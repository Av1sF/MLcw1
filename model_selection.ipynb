{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection \n",
    "\n",
    "In this notebook, I will be experimenting and evaluating the baseline performance of different models using one-hot encoding. As one-hot encoding gives more information about categorical data's correlation to outcome. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LinearRegression \n",
    "from sklearn.ensemble import RandomForestRegressor \n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.svm import SVR\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading in Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoded correlation \n",
    "\n",
    "# Load dataset \n",
    "ds = pd.read_csv('./CW1_train.csv')\n",
    "\n",
    "# Identify categorical columns\n",
    "categorical_cols = ['cut', 'color', 'clarity']  # Replace with actual categorical column names\n",
    "\n",
    "# One-hot encode categorical variables\n",
    "ds = pd.get_dummies(ds, columns=categorical_cols, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create splits \n",
    "train, test = train_test_split(ds, test_size=0.2, random_state=123)\n",
    "X_train = train.drop(columns=['outcome'])\n",
    "y_train = train['outcome']\n",
    "X_test = test.drop(columns=['outcome'])\n",
    "y_test = test['outcome']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Evaluation Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R2 score \n",
    "def r2_fn(y_test, y_pred):\n",
    "    eps = y_test - y_pred\n",
    "    rss = np.sum(eps ** 2)\n",
    "    tss = np.sum((y_test - y_test.mean()) ** 2)\n",
    "    r2 = 1 - (rss / tss)\n",
    "    return r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experimenting with linear regression is simpler than other methods. Here we experiment with the performance without normalisation (using z-score) and transformation and with. Normalisation was considered as it helps to avoid the vanishing gradient problem during training, furthermore the transformation such as log and square root helps reduce skew. This in turn aids in fufilling the normality assumption of linear regression models better. \n",
    "\n",
    "Log transformation was applied to carat and price to reduce skew. However, a square root transformation was applied to y for the same purpose, but due to zero-values this was more appropriate. \n",
    "\n",
    "From observing the R2 score below, normalisation and without normalisation did not have an visible impact. This was expected as it was hypothesised that most/nearly all features were not linearly correlated from observing the scatterplots.  Hence, due to it's simplistic structure, assuming noramlity and linearity, this model had a really poor performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without normalisation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2891358294062252\n",
      "10.656523176745546\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegression()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate \n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Scorers \n",
    "print(r2_fn(y_test, y_pred))\n",
    "print(root_mean_squared_error(y_pred=y_pred, y_true=y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = X_train.copy()\n",
    "X_test_scaled = X_test.copy()\n",
    "y_train_scaled = y_train.copy()\n",
    "y_test_scaled = y_test.copy()\n",
    "\n",
    "# Log and square root transformations \n",
    "X_train_scaled['carat'] = np.log(X_train['carat'])\n",
    "X_train_scaled['price'] = np.log(X_train['price'])\n",
    "X_train_scaled['y'] = np.sqrt(X_train['y'])\n",
    "\n",
    "X_test_scaled['carat'] = np.log(X_test['carat'])\n",
    "X_test_scaled['price'] = np.log(X_test['price'])\n",
    "X_test_scaled['y'] = np.sqrt(X_test['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard score normalisation \n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "y_train_scaled = scaler.fit_transform(y_train.to_frame())\n",
    "y_test_scaled = scaler.transform(y_test.to_frame())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2891358294061971\n",
      "0.8363255507891988\n"
     ]
    }
   ],
   "source": [
    "# Fit model with transformed normalised data\n",
    "model = LinearRegression()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_scaled, y_train_scaled)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_scaled = model.predict(X_test_scaled)\n",
    "\n",
    "# Scorers \n",
    "print(r2_fn(y_test_scaled, y_pred_scaled))\n",
    "print(root_mean_squared_error(y_pred=y_pred_scaled, y_true=y_test_scaled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernal Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although, this model is more flexible than linear regression. It did not perform well on both normalised transform data and the raw data. The poor performance further supports the fact that features are not linearly correlated. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-normalised data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2569791821786881\n",
      "10.894886548521145\n"
     ]
    }
   ],
   "source": [
    "kr_model = KernelRidge()  \n",
    "\n",
    "# Train the model\n",
    "kr_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate \n",
    "y_pred = kr_model.predict(X_test)\n",
    "\n",
    "# Scorers \n",
    "print(r2_fn(y_test, y_pred))\n",
    "print(root_mean_squared_error(y_pred=y_pred, y_true=y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalised and transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.289145238838981\n",
      "0.8363200157127226\n"
     ]
    }
   ],
   "source": [
    "kr_model = KernelRidge()\n",
    "\n",
    "# Train the model\n",
    "kr_model.fit(X_train_scaled, y_train_scaled)\n",
    "\n",
    "# Evaluate \n",
    "y_pred_scaled = kr_model.predict(X_test_scaled)\n",
    "\n",
    "# Scorers \n",
    "print(r2_fn(y_test_scaled, y_pred_scaled))\n",
    "print(root_mean_squared_error(y_pred=y_pred_scaled, y_true=y_test_scaled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As Random Forest (RF) is based on partitioning the data instead of comparing feature values for predictions, it does not require normalisation. However, the nearly double improvement in R2 performance compared to linear regression models indicates that features are related nonlinearly. \n",
    "\n",
    "Furthermore, RFs are more robust to noise and outliers , which works really well with our dataset. As previously from the EDA, we observed noisy variables such as a1 during the histogram plot and calculated the number of outliers for each variable in the boxplots. RFs also have the ability to calculate a feature's importance which may help better priorise important features even after feature selection. \n",
    "\n",
    "However, it is important to note that RFs could overfit. Meaning careful validation processes have to be used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4197266709518436\n",
      "9.628054191787001\n"
     ]
    }
   ],
   "source": [
    "rf_model = RandomForestRegressor()\n",
    "\n",
    "# Train the model\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Scorers \n",
    "print(r2_fn(y_test, y_pred))\n",
    "print(root_mean_squared_error(y_pred=y_pred, y_true=y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, Support Vector Regressor (SVR) did not perform well at all, as it is a type of linear model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-normalised Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0016752197724354545\n",
      "12.649859043169714\n"
     ]
    }
   ],
   "source": [
    "svr_model = SVR() \n",
    "\n",
    "# Train the model\n",
    "svr_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate \n",
    "y_pred = svr_model.predict(X_test)\n",
    "\n",
    "# Scorers \n",
    "print(r2_fn(y_test, y_pred))\n",
    "print(root_mean_squared_error(y_pred=y_pred, y_true=y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalised and Transformed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ROG\\MLcw1\\.conda\\Lib\\site-packages\\sklearn\\utils\\validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2757.873224353075\n",
      "0.8202511178469603\n"
     ]
    }
   ],
   "source": [
    "svr_model = SVR() \n",
    "\n",
    "# Train the model\n",
    "svr_model.fit(X_train_scaled, y_train_scaled)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_scaled = svr_model.predict(X_test_scaled)\n",
    "\n",
    "# Scorers \n",
    "print(r2_fn(y_test_scaled, y_pred_scaled))\n",
    "print(root_mean_squared_error(y_pred=y_pred_scaled, y_true=y_test_scaled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XgBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experimenting with another type non-linear regression model, gradient boosting. XGBoost presented itself as a great potential model. Yet again, because XGboost is essentially an ensemble algorithm composed of decision trees, normalisation was not required. XGBoost is also robust to outliers and noise well. \n",
    "\n",
    "As such, we can observe a relatively high R2 score with this model too! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.39304559866614586\n",
      "9.846916467093138\n"
     ]
    }
   ],
   "source": [
    "xgboost_model = XGBRegressor()\n",
    "\n",
    "# Train the model\n",
    "xgboost_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate \n",
    "y_pred = xgboost_model.predict(X_test)\n",
    "\n",
    "# Scorers \n",
    "print(r2_fn(y_test, y_pred))\n",
    "print(root_mean_squared_error(y_pred=y_pred, y_true=y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion \n",
    "\n",
    "In conclusion, random forests and gradient boosting with XGBoost both had the higher performance than the other linear models.\n",
    "\n",
    "Random forests had a higher performance difference of 0.02668. Weighing the advantages of both models, I decided to continue with feature selection and hypertuning on XGBoost. \n",
    "\n",
    "As both models were robust to noise and outliers, it had a high baseline performance and was able to adapt to the non-linear nature of the data. According to [XGBoosting articles](https://xgboosting.com/xgboost-vs-random-forest/), XGBoosting tends to train faster and it also has built in regularisation techniques to help prevent overfitting. Random forests hyperparameter sensitivity is less sensitive than XGBoost. High sensitivity during tuning could help us reach and maximise performance further with XGBoost. Even though, random forest excels with parallelisation it is not a factor that needs to be priortised in this project. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
